# -*- coding: utf-8 -*-
"""write_frame_info2csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19kVTkdWRx1YL4gyB9EkwniV7q3AMKc5_

make a .csv file to save jpeg file index and make a row for the correspounding vector.
"""
#--- to do: a image embedding is about 1000 dim, so may be we need to reduce the dimension using PCA or select k most  important feature.
# --- to do: use faiss-gpu.
# --- to do: batch produce the embedding, especially for VGG16.
# -- to do: using gpu to embedding

# script to generate embeddings and caculate Laplacian Variance.
import cv2
import pandas as pd
import torch
# from transformers import AutoImageProcessor, EfficientNetModel, ViTModel, AutoModel, CLIPProcessor, CLIPModel, Blip2Processor, Blip2Model
# from torchvision import models, transforms
import numpy as np
import os
import sys
sys.path.append('/root/autodl-tmp/AttieCode')
import re
import faiss
import csv
from PIL import Image
from Config.config import CONF
# from multiprocessing import Pool

#chose a device to run the code
device = "cuda:0" if torch.cuda.is_available() else "cpu"
 
if CONF.RGB_EMBEDDING.USING_VGG:
    from torchvision import models, transforms
    model = models.vgg16()
    state_dict = torch.load( os.path.join(CONF.PATH.MODELS, "vgg16.pth"), map_location='cpu' )
    model.load_state_dict(state_dict)
if CONF.RGB_EMBEDDING.USING_BLIP2:
    from transformers import Blip2Processor, Blip2Model
    image_processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
    model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

model.to(device)

def load_image(image_path):
    # Check if the image exists in the folder
    if os.path.exists(image_path):
        img = Image.open(image_path)

        # Convert image to RGB if not already in that mode
        if img.mode != 'RGB':
            img = img.convert('RGB')
        # Resize the image
        new_size = (224, 224)
        resized_img = img.resize(new_size, Image.Resampling.LANCZOS)

        return resized_img
    else:
        print(f"Image not found.")
        return None

"""make image embedding"""

"""calculate Laplacian varicance"""

def  Laplacian_varicance(image_path):
    """
    Detect if an image is blurry using the Laplacian variance method.

    Args:
        image_path (numpy.ndarray): The input image

    Returns:
        bool: True if the image is blurry, False otherwise.
        float: The variance of the Laplacian.
    """
    import cv2

    img_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)
    if img_bgr is None:
        raise FileNotFoundError(image_path)            # :contentReference[oaicite:8]{index=8}
    img_rgb   = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img_rgb, (224,224), interpolation=cv2.INTER_LANCZOS4)
    gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)

    # Compute the Laplacian of the image
    laplacian = cv2.Laplacian(gray, cv2.CV_64F)

    # Compute the variance of the Laplacian
    variance = laplacian.var()

    # Determine if the image is blurry
    return variance #laplacian,  variance < threshold,

#Blip 2
def extract_features_blip(file_path):
    # image_processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
    # model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
    img = load_image(file_path)
    img.to(device)
    inputs = image_processor(images=img, return_tensors='pt', padding=True)
    print('input shape: ', inputs['pixel_values'].shape)

    with torch.no_grad():
        outputs = model.get_qformer_features(**input)
    embedding = outputs.last_hidden_state
    embedding = embedding[:, 0, :].squeeze(1)
    return embedding.numpy()

#vgg16
def extract_features_vgg16(file_path, batch_size = 64):

    model.eval()  # Set the model to evaluation mode

    # Define the transformation to preprocess the image
    preprocess = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    img = load_image(file_path)
    img_t = preprocess(img)
    img_t.to(device)
    batch_t = torch.unsqueeze(img_t, 0).to(device)
    # print("batch_t.device:", batch_t.device)
    # print(model.device)
    with torch.no_grad():
        embedding = model(batch_t)
    return embedding.cpu().numpy()

def select_frames(folder_path, frame_size):
    '''
    Select frame through RGB file, which is based on the cosine similarity of image embedding
    and Laplacian variance.
    '''
    # Define the file name
    file_name = 'jpg_embeding_info.csv'
    file_path = os.path.join(folder_path, file_name)

    Laplacian_varicances = []; embeddings = []
    #rows = [ ['frame_index', 'Laplacian_varicance', 'jpg_embedding'], [the next frame]]
    
    for i in range(frame_size):
        # Open the image file and read its data
        filename = 'frame-' + str(i).zfill(6) + ".color.jpg"
        image_file_path = os.path.join(folder_path, filename)

        #Get Laplacian variance
        lap_info = Laplacian_varicance(image_file_path)
        Laplacian_varicances.append( float(lap_info) )

        #Get frame embedding
        # embedding = extract_features_blip(image_file_path)
        embedding = extract_features_vgg16(image_file_path)
        embeddings.append( embedding.reshape(-1) )
    
    Laplacian_varicances = np.array(Laplacian_varicances, dtype = np.float32)
    embeddings = np.array(embeddings, dtype = np.float32)

    #Normalize the embedding 4 Cosine similarity
    embeddings = embeddings / np.linalg.norm(embeddings, axis = -1, keepdims = True)

    #Create a FAISS index for similarity search
    n, dim = embeddings.shape[0], embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    #Add all features to the FAISS
    index.add(embeddings)

    #search for the top k photoes most alike photoes.
    scores, top_k_idx = index.search(embeddings, CONF.RGB_EMBEDDING.K+1)  #k+1 to exclude itself

    #exclude one self
    neighbor_idxs = []
    neighbor_sims = []
    for i in range(n):
        row_idxs = top_k_idx[i]
        row_sims = scores[i]

        mask = (row_idxs != i)

        filtered_idxs = row_idxs[mask][:CONF.RGB_EMBEDDING.K]
        filtered_sims = row_sims[mask][:CONF.RGB_EMBEDDING.K]
        neighbor_idxs.append(filtered_idxs)
        neighbor_sims.append(filtered_sims)
    neighbor_idxs = np.array(neighbor_idxs)  # shape (N, K)
    neighbor_sims = np.array(neighbor_sims)  # shape (N, K)

    # 5. filter vague frames that are similar to near by frames.
    # Determine frames that are blurry
    is_blurry = Laplacian_varicances <= CONF.RGB_EMBEDDING.LAPLACIAN_VARIANCE_THRESHOLD

    # Determine frames that have at least one similar neighbor
    has_similar_neighbor = np.any(neighbor_sims >= CONF.RGB_EMBEDDING.SIMILARITY_THERSHOLD, axis=1)

    # Exclude frames that are both blurry and have at least one similar neighbor
    keep_mask = ~(is_blurry & has_similar_neighbor)

    # 6. return selected frame list
    return np.nonzero(keep_mask)[0].tolist()
